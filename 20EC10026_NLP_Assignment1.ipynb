{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwrGjNRegaaP",
        "outputId": "e785bd17-a9f0-42bf-9817-4cec56624938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Package treebank is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk import pos_tag\n",
        "from sklearn.svm import SVC\n",
        "from nltk.corpus import treebank\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import defaultdict\n",
        "from nltk.corpus import movie_reviews\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('treebank')\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"movie_reviews\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part1: POS Tagger for Treebank Corpus"
      ],
      "metadata": {
        "id": "exAQgZazBzgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_sents = treebank.tagged_sents()                                        # Creating word vocabulary and tag set\n",
        "\n",
        "words = set()\n",
        "tags = set()\n",
        "\n",
        "for sentence in treebank_sents:\n",
        "  for word, tag in sentence:\n",
        "    words.add(word)\n",
        "    tags.add(tag)\n",
        "\n",
        "tags.add('Start')\n",
        "tags.add('End')\n",
        "\n",
        "word2idx = {word: idx for idx, word in enumerate(words)}                         # Word-to-index and Index-to-Word Maps\n",
        "idx2word = {idx: word for idx, word in enumerate(words)}\n",
        "\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(tags)}                             # Tag-to-index and Index-to-Tag Maps\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(tags)}"
      ],
      "metadata": {
        "id": "D2Knl97ORGpf"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Model States, Viterbi Algorithm, and Smoothing for training on Different Corpora"
      ],
      "metadata": {
        "id": "KEK7S9fxB9K4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def HMM_States(corpus):\n",
        "  emission_counts = defaultdict(lambda: defaultdict(int))                       # Creating the emission and transition probability dictionaries by iterating through all the sentences in the corpus and calculating counts of emission\n",
        "  tag_counts = defaultdict(int)                                                 # of words given tags and also transition from one tag to another for all (tag, word) & (prev_tag, curr_tag) pairs possible respectively\n",
        "  emission_prob = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "  for sentence in corpus:\n",
        "    for word, tag in sentence:\n",
        "      emission_counts[tag][word] += 1\n",
        "      tag_counts[tag] += 1\n",
        "\n",
        "  for sentence in corpus:\n",
        "    for word, tag in sentence:\n",
        "      emission_prob[tag][word] = emission_counts[tag][word]/tag_counts[tag]\n",
        "\n",
        "  transition_counts = defaultdict(lambda: defaultdict(int))\n",
        "  start_tag_counts = defaultdict(int)\n",
        "  transition_prob = defaultdict(lambda: defaultdict(float))\n",
        "\n",
        "  for sentence in corpus:                                                       # Calculating total number of tags that can appear right after Start\n",
        "    prev_tag = None\n",
        "    for _, tag in sentence:\n",
        "      if prev_tag is not None:\n",
        "        transition_counts[prev_tag][tag] += 1\n",
        "      else:\n",
        "        start_tag_counts[tag] += 1\n",
        "      prev_tag = tag\n",
        "\n",
        "  total_start_count = sum(start_tag_counts.values())\n",
        "\n",
        "  for tag in tags:\n",
        "    transition_prob['Start'][tag] = start_tag_counts[tag]/total_start_count     # Transition probability of Start -> Tag = count[tag] at start / total tags possible at start\n",
        "\n",
        "  for prev_tag in transition_counts:\n",
        "    total_transition_count = sum(transition_counts[prev_tag].values())\n",
        "    for tag in transition_counts[prev_tag]:\n",
        "      transition_prob[prev_tag][tag] = transition_counts[prev_tag][tag]\n",
        "      transition_prob[prev_tag][tag] /= total_transition_count\n",
        "\n",
        "  return emission_prob, transition_prob\n",
        "\n",
        "\n",
        "def Viterbi(tokenized_sent, emission_prob, transition_prob):\n",
        "  tokenized_sent.append('End')\n",
        "  VITERBI = np.zeros((len(tags), len(tokenized_sent)+1))                        # VITERBI.shape = (len(tags)+2, len(tokenized_sent)+1), but since 'Start' and 'End' have already been added to tags and also 'End' has been appended to sent\n",
        "  BACKPOINTER = np.zeros((len(tokenized_sent)))\n",
        "\n",
        "  VITERBI[tag2idx['Start']][0] = 1                                               # The value of Start tag in the 1st column should be set to 1 in the VITERBI Matrix\n",
        "\n",
        "  for i in range(1, len(tokenized_sent)+1):\n",
        "    maxV = np.max(VITERBI[:, i-1])                                              # Starting from column number 2, maxV gets the maximum value present in the i-1th column of the VITERBI Matrix, and argV gets its index to be passed to the\n",
        "    argV = np.argmax(VITERBI[:, i-1])                                           # BACKPOINTER to trace back the most probable POS tags path\n",
        "    BACKPOINTER[i-1] = argV\n",
        "    for j in range(len(tags)):\n",
        "      VITERBI[j][i] = maxV\n",
        "      VITERBI[j][i] *= transition_prob[idx2tag[argV]][idx2tag[j]];\n",
        "      VITERBI[j][i] *= emission_prob[idx2tag[j]][tokenized_sent[i-1]];\n",
        "\n",
        "  return BACKPOINTER\n",
        "\n",
        "def apply_smoothing(emission_prob, new_words, tags, smoothing_factor=0.05):     # Added some smoothing to handle unseen words from Movie Dataset\n",
        "  smoothed_emission_prob = {tag: dict(emission_prob[tag]) for tag in tags}\n",
        "\n",
        "  for tag in tags:\n",
        "    for word in new_words:\n",
        "      if word not in smoothed_emission_prob[tag]:\n",
        "        smoothed_emission_prob[tag][word] = smoothing_factor #/ (smoothing_factor * len(emission_prob[tag]) + len(new_words))\n",
        "  return smoothed_emission_prob"
      ],
      "metadata": {
        "id": "U26Lzu-Mk_u2"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "treebank_pos_tags = [' '.join([f\"{tag}\" for word, tag in tagged_sentence]) for tagged_sentence in treebank.tagged_sents()]\n",
        "emission_prob, transition_prob = HMM_States(treebank_sents)"
      ],
      "metadata": {
        "id": "qIKd_aJnBgFR"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generating POS tags for 10 sentences in the treebank corpus"
      ],
      "metadata": {
        "id": "-Yagr6s-Bnb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in range(10):\n",
        "  tokens = treebank.sents()[sent]\n",
        "  BACKPOINTER = Viterbi(tokens, emission_prob, transition_prob)\n",
        "  decoded_tag_seq = \" \".join([idx2tag[BACKPOINTER[i]] for i in range(1, len(BACKPOINTER))])\n",
        "\n",
        "  sentence = \" \".join(treebank.sents()[sent])\n",
        "\n",
        "  print(f\"Sentence        : {sentence}\\nActual Sequence : {treebank_pos_tags[sent]}\\nDecoded Sequence: {decoded_tag_seq}\\n\\n\")"
      ],
      "metadata": {
        "id": "Q7x2U7F_obkr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2dddd111-ad1d-41eb-80eb-f6584fb84875"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence        : Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n",
            "Actual Sequence : NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD .\n",
            "Decoded Sequence: NNP NNP , CD NNS JJ , MD VB DT NN IN DT JJ NN NNP CD .\n",
            "\n",
            "\n",
            "Sentence        : Mr. Vinken is chairman of Elsevier N.V. , the Dutch publishing group .\n",
            "Actual Sequence : NNP NNP VBZ NN IN NNP NNP , DT NNP VBG NN .\n",
            "Decoded Sequence: NNP NNP VBZ NN IN NNP NNP , DT JJ NN NN .\n",
            "\n",
            "\n",
            "Sentence        : Rudolph Agnew , 55 years old and former chairman of Consolidated Gold Fields PLC , was named *-1 a nonexecutive director of this British industrial conglomerate .\n",
            "Actual Sequence : NNP NNP , CD NNS JJ CC JJ NN IN NNP NNP NNP NNP , VBD VBN -NONE- DT JJ NN IN DT JJ JJ NN .\n",
            "Decoded Sequence: NNP NNP , CD NNS JJ CC JJ NN IN NNP NNP NNP NNP , VBD VBN -NONE- DT JJ NN IN DT JJ JJ NN .\n",
            "\n",
            "\n",
            "Sentence        : A form of asbestos once used * * to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed * to it more than 30 years ago , researchers reported 0 *T*-1 .\n",
            "Actual Sequence : DT NN IN NN RB VBN -NONE- -NONE- TO VB NNP NN NNS VBZ VBN DT JJ NN IN NN NNS IN DT NN IN NNS VBN -NONE- TO PRP RBR IN CD NNS IN , NNS VBD -NONE- -NONE- .\n",
            "Decoded Sequence: DT NN IN NN RB VBN -NONE- -NONE- TO VB NNP NN NNS VBZ VBN DT JJ NN IN NN NNS IN DT NN IN NNS VBN -NONE- TO PRP JJR IN CD NNS IN , NNS VBD -NONE- -NONE- .\n",
            "\n",
            "\n",
            "Sentence        : The asbestos fiber , crocidolite , is unusually resilient once it enters the lungs , with even brief exposures to it causing symptoms that *T*-1 show up decades later , researchers said 0 *T*-2 .\n",
            "Actual Sequence : DT NN NN , NN , VBZ RB JJ IN PRP VBZ DT NNS , IN RB JJ NNS TO PRP VBG NNS WDT -NONE- VBP RP NNS JJ , NNS VBD -NONE- -NONE- .\n",
            "Decoded Sequence: DT NN NN , NN , VBZ RB JJ RB PRP VBZ DT NNS , IN RB JJ NNS TO PRP VBG NNS IN -NONE- VBP RP NNS JJ , NNS VBD -NONE- -NONE- .\n",
            "\n",
            "\n",
            "Sentence        : Lorillard Inc. , the unit of New York-based Loews Corp. that *T*-2 makes Kent cigarettes , stopped using crocidolite in its Micronite cigarette filters in 1956 .\n",
            "Actual Sequence : NNP NNP , DT NN IN JJ JJ NNP NNP WDT -NONE- VBZ NNP NNS , VBD VBG NN IN PRP$ NN NN NNS IN CD .\n",
            "Decoded Sequence: NNP NNP , DT NN IN NNP NNP NNP NNP IN -NONE- VBZ NNP NNS , VBD VBG NN IN PRP$ NN NN NNS IN CD .\n",
            "\n",
            "\n",
            "Sentence        : Although preliminary findings were reported *-2 more than a year ago , the latest results appear in today 's New England Journal of Medicine , a forum likely * to bring new attention to the problem .\n",
            "Actual Sequence : IN JJ NNS VBD VBN -NONE- RBR IN DT NN IN , DT JJS NNS VBP IN NN POS NNP NNP NNP IN NNP , DT NN JJ -NONE- TO VB JJ NN TO DT NN .\n",
            "Decoded Sequence: IN JJ NNS VBD VBN -NONE- RBR IN DT NN IN , DT JJS NNS VBP IN NN POS NNP NNP NNP IN NNP , DT NN JJ -NONE- TO VB JJ NN TO DT NN .\n",
            "\n",
            "\n",
            "Sentence        : A Lorillard spokewoman said , `` This is an old story .\n",
            "Actual Sequence : DT NNP NN VBD , `` DT VBZ DT JJ NN .\n",
            "Decoded Sequence: DT NNP NN VBD , `` DT VBZ DT JJ NN .\n",
            "\n",
            "\n",
            "Sentence        : We 're talking about years ago before anyone heard of asbestos having any questionable properties .\n",
            "Actual Sequence : PRP VBP VBG IN NNS IN IN NN VBD IN NN VBG DT JJ NNS .\n",
            "Decoded Sequence: PRP VBP VBG IN NNS IN IN NN VBD IN NN VBG DT JJ NNS .\n",
            "\n",
            "\n",
            "Sentence        : There is no asbestos in our products now . ''\n",
            "Actual Sequence : EX VBZ DT NN IN PRP$ NNS RB . ''\n",
            "Decoded Sequence: EX VBZ DT NN IN PRP$ NNS RB . ''\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 2: Vanilla Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "ycI0qqToCT5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()                                                       # Method to pre-process the text by doing stemming and removing stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def preprocess_text(text):\n",
        "  words = nltk.word_tokenize(text.lower())\n",
        "  words = [stemmer.stem(word) for word in words if word.isalnum() and word not in stop_words]\n",
        "  return ' '.join(words)"
      ],
      "metadata": {
        "id": "JcZU17UT0Qth"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):                                                           # Method to remove special characters and eliminate white space\n",
        "  pattern = r'[^A-Za-z0-9\\s]'\n",
        "  cleaned_text = re.sub(pattern, '', text)\n",
        "  cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
        "  cleaned_text = cleaned_text.strip()\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "qHH4A1EF0mmR"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]    # Loading the dataset\n",
        "random.shuffle(documents)\n",
        "\n",
        "texts = [' '.join(document) for document, _ in documents]\n",
        "labels = [label for _, label in documents]\n",
        "\n",
        "processed_text = [preprocess_text(clean_text(text)) for text in texts]"
      ],
      "metadata": {
        "id": "Jgus5JvYZj7f"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the Vanilla Sentiment Analysis Model"
      ],
      "metadata": {
        "id": "l2kUdai10tuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(processed_text, labels, test_size=0.4, random_state=42) # For the Vanilla sentiment analysis model, we split the data into train-test-val in the ratio 4:3:3\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "# Finally we use tf-idf to create word embeddings                               # tf-idf gave better results as compared to word2vec and glove. This is probably because this dataset doesn't require sequential information (context). Otherwise\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)                           # we would have had to use another model which can work with sequential data, like LSTM or Transformers\n",
        "X_train = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_val = tfidf_vectorizer.transform(X_val)\n",
        "X_test = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "classifier = SVC()                                                              # Using a standard Support Vector Machine (gave better results as compared to Logit, Naive Bayes and Decision Trees)\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "val_predictions = classifier.predict(X_val)\n",
        "\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "val_classification_report = classification_report(y_val, val_predictions)\n",
        "\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "print(\"Validation Classification Report:\\n\", val_classification_report)\n",
        "\n",
        "test_predictions = classifier.predict(X_test)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_classification_report = classification_report(y_test, test_predictions)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Classification Report:\\n\", test_classification_report)"
      ],
      "metadata": {
        "id": "jmjtuCE0kIWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3134740-9400-4ee3-dc81-8f98b4cf81bd"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.8175\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.82      0.83      0.83       211\n",
            "         pos       0.81      0.80      0.81       189\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "Test Accuracy: 0.8075\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.80      0.80      0.80       190\n",
            "         pos       0.82      0.81      0.82       210\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Part 3: POS Tagging of Movie Dataset"
      ],
      "metadata": {
        "id": "E046mnMaDorr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Since the POS model is trained on the Treebank corpus, we need to apply smoothing to the emission_prob matrix for it to be able to better handle unseen words from the new corpus (Movie Dataset)"
      ],
      "metadata": {
        "id": "WUFT5N7NDkzD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###We notice that the performance of the Viterbi POS Tagger is quite poor on the movie reviews dataset. A possible explanation of this is that even after smoothing, the unseen words have very little probability and since the sentences are very long, it results in the probabilities multiplying and becoming very close to 0, due to which the tag starts to converge towards tag with tag index 0"
      ],
      "metadata": {
        "id": "zO9UFIEq2AnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in texts:\n",
        "  for word in sentence.split():\n",
        "    words.add(word)\n",
        "\n",
        "emission_prob_smooth = apply_smoothing(emission_prob, words, tags)\n",
        "print(emission_prob == emission_prob_smooth)\n",
        "\n",
        "sent = texts[1]\n",
        "sent_tokens = sent_tokenize(sent)\n",
        "for sents in sent_tokens:\n",
        "  token = word_tokenize(sents)\n",
        "  BACKPOINTER = Viterbi(token, emission_prob_smooth, transition_prob)\n",
        "  decoded_tag_seq = \" \".join([idx2tag[BACKPOINTER[i]] for i in range(1, len(BACKPOINTER))])\n",
        "  print(token)\n",
        "  print(decoded_tag_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Heb16N28COjI",
        "outputId": "37af3ba1-87b1-456a-ffa2-ee604ac7b2b2"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "['martin', 'scorsese', \"'\", 's', 'triumphant', 'adaptation', 'of', 'edith', 'wharton', \"'\", 's', 'the', 'age', 'of', 'innocence', 'is', 'a', 'stunning', 'film', 'for', 'the', 'quintessential', 'new', 'york', 'filmmaker', ',', 'the', 'man', 'who', 'brought', 'the', 'streets', 'of', 'taxi', 'driver', 'and', 'mean', 'streets', 'to', 'life', '.', 'End']\n",
            "DT NN IN DT NN IN IN DT NN IN DT DT JJ IN DT VBZ DT NN IN DT DT NN JJ NN IN , DT JJ NN IN DT NN IN DT JJ CC NNP NNP TO VB .\n",
            "['it', 'seems', 'like', 'an', 'odd', 'choice', 'for', 'scorsese', 'to', 'do', 'a', 'period', 'piece', 'in', 'the', 'early', '1900', \"'\", 's', ',', 'but', 'the', 'fact', 'that', 'he', 'pulls', 'it', 'off', 'so', 'brilliantly', 'is', 'a', 'wonder', ',', 'and', 'a', 'testament', 'to', 'the', 'greatness', 'of', 'scorsese', 'as', 'a', 'filmmaker', '.', 'End']\n",
            "PRP VBD -NONE- DT NN IN DT NN TO VB DT JJ NNS IN DT NN IN DT NN , NNP DT JJ IN DT NN PRP VBD -NONE- TO MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD\n",
            "['this', 'is', 'a', 'gorgeous', 'visual', 'experience', 'that', 'it', 'surely', 'one', 'of', 'scorsese', \"'\", 's', 'finest', '.', 'End']\n",
            "DT VBZ DT NN IN DT IN PRP VBD -NONE- IN DT NN IN DT .\n",
            "['newland', 'archer', '(', 'day', '-', 'lewis', ')', 'is', 'a', 'prestigious', 'lawyer', 'who', 'is', 'engaged', 'to', 'may', 'welland', '(', 'ryder', ')', ',', 'a', 'somewhat', 'empty', 'and', 'shallow', 'new', 'yorker', ',', 'who', 'belongs', 'to', 'a', 'prestigious', 'family', 'and', 'is', 'quite', 'beautiful', '.', 'End']\n",
            "DT NN IN DT NN IN DT VBZ DT NN IN DT VBZ -NONE- TO VB DT NN IN DT , DT NN IN CC NNP JJ NN , WP -NONE- TO DT NN IN CC VBZ -NONE- TO MD\n",
            "['the', 'marriage', 'is', 'one', 'which', 'will', 'unite', 'two', 'very', 'prestigious', 'families', ',', 'in', 'a', 'society', 'where', 'nothing', 'is', 'more', 'important', 'than', 'the', 'opinions', 'of', 'others', '.', 'End']\n",
            "DT NN VBZ -NONE- TO NN IN DT NN IN DT , IN DT JJ NN IN VBZ RBR IN IN DT NN IN DT .\n",
            "['on', 'the', 'day', 'that', 'archer', 'is', 'to', 'announce', 'his', 'engagement', 'to', 'may', ',', 'countess', 'ellen', 'olenska', '(', 'pfeiffer', ')', ',', 'cousin', 'of', 'may', ',', 'walks', 'into', 'archer', \"'\", 's', 'life', '.', 'End']\n",
            "DT DT JJ IN DT VBZ TO DT NN IN TO VB , NNP NNP NNP NNP NNP NNP , NNP IN DT , NNP NNP NNP NNP NNP NNP .\n",
            "['archer', 'is', 'immediately', 'captivated', ',', 'and', 'finds', 'himself', 'in', 'love', 'with', 'ellen', '.', 'End']\n",
            "DT VBZ -NONE- TO MD MD MD MD MD MD MD MD MD\n",
            "['archer', 'is', 'also', 'bound', 'by', 'the', 'limits', 'of', 'new', 'york', 'society', ',', 'which', 'is', 'an', 'intrusive', 'as', 'any', 'other', 'in', 'the', 'world', '.', 'End']\n",
            "DT VBZ -NONE- TO VB DT NN IN JJ NN IN , WDT VBZ DT NN IN DT NN IN DT JJ .\n",
            "['archer', 'finds', 'himself', 'having', 'a', 'secret', 'love', 'affair', 'in', 'his', 'mind', 'with', 'countess', 'olenska', ',', 'attempting', 'to', 'keep', 'her', 'in', 'his', 'mind', 'while', 'trying', 'not', 'to', 'lose', 'his', 'social', 'status', '.', 'End']\n",
            "DT NN IN VBG DT JJ NN IN IN DT JJ IN DT NN , NNP TO DT NN IN DT JJ NNS IN DT TO DT NN IN DT .\n",
            "['the', 'film', \"'\", 's', 'subject', 'matter', 'may', 'seem', 'alien', 'to', 'scorsese', ',', 'but', 'the', 'theme', 'is', 'definitely', 'not', '.', 'End']\n",
            "DT JJ NN IN DT JJ NN IN DT TO VB , NNP DT JJ VBZ -NONE- TO MD\n",
            "['it', 'is', 'a', 'theme', 'of', 'forbidden', 'romance', ',', 'guilty', 'pleasures', ',', 'and', 'the', 'consequences', 'causes', 'because', 'of', 'those', 'actions', '.', 'End']\n",
            "PRP VBZ DT JJ IN DT JJ , NNP NNP , CC DT NN IN DT IN NNP NNP .\n",
            "['there', 'is', 'a', 'painstakingly', 'flawed', 'hero', ',', 'and', 'his', 'choice', 'between', 'the', 'life', 'he', 'wants', ',', 'and', 'the', 'life', 'he', 'is', 'destined', 'for', '.', 'End']\n",
            "DT VBZ DT NN IN DT , CC NNP NNP NNP DT JJ NN IN , CC DT JJ NN VBZ -NONE- IN .\n",
            "['in', 'truth', ',', 'it', 'is', 'a', 'film', 'about', 'a', 'society', 'the', 'audience', 'doesn', \"'\", 't', 'know', 'about', ',', 'but', 'wants', 'to', 'find', 'out', 'more', ',', 'much', 'like', 'the', 'society', 'of', 'goodfellas', 'or', 'even', 'kundun', '.', 'End']\n",
            "IN DT , PRP VBZ DT JJ IN DT JJ DT JJ NN IN DT NN IN , NNP NNP TO DT NN JJR , NNP NNP DT JJ IN DT NN RB IN .\n",
            "['the', 'performances', 'are', 'absolutely', 'breathtaking', '.', 'End']\n",
            "DT NN , NNP NNP .\n",
            "['day', '-', 'lewis', 'portrays', 'more', 'mental', 'anguish', 'in', 'his', 'face', 'than', 'one', 'man', 'should', 'be', 'forced', 'to', 'take', '.', 'End']\n",
            "DT NN IN DT JJR IN DT IN DT JJ IN DT JJ NN IN DT TO VB .\n",
            "['pfeiffer', 'is', 'marvelous', 'as', 'countess', 'olenska', ',', 'a', 'mix', 'of', 'passion', 'and', 'beauty', 'that', 'the', 'audience', 'would', 'die', 'for', 'as', 'well', '.', 'End']\n",
            "DT VBZ -NONE- IN DT NN , DT JJ IN DT CC NNP IN DT JJ NN IN DT IN DT .\n",
            "['ryder', 'is', 'probably', 'the', 'gem', 'of', 'the', 'group', ',', 'for', 'it', 'is', 'her', 'quiet', 'presence', 'that', 'overwhelms', 'the', 'plot', ',', 'and', 'slowly', 'pushes', 'day', '-', 'lewis', 'closer', 'and', 'closer', 'to', 'his', 'eventual', 'ending', '.', 'End']\n",
            "DT VBZ -NONE- DT NN IN DT NN , NNP PRP VBZ -NONE- TO VB IN DT DT JJ , CC NNP NNP NNP NNP NNP NNP CC NNP TO VB DT NN .\n",
            "['the', 'supporting', 'cast', 'is', 'also', 'wonderful', ',', 'with', 'several', 'characters', 'so', 'singular', 'that', 'they', 'are', 'indelible', 'in', 'one', \"'\", 's', 'memory', '.', 'End']\n",
            "DT NN IN VBZ -NONE- TO MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD\n",
            "['scorsese', 'definitely', 'has', 'a', 'passion', 'for', 'filmmaking', '.', 'End']\n",
            "DT NN VBZ DT JJ NN IN .\n",
            "['his', 'lavish', 'and', 'sumptuous', 'set', 'design', 'and', 'marvelous', 'recreation', 'of', 'new', 'york', 'is', 'a', 'wondrous', 'sight', '.', 'End']\n",
            "DT NN CC NNP NNP NNP CC NNP NNP IN JJ NN VBZ DT NN IN .\n",
            "['he', 'literally', 'transports', 'the', 'viewer', 'to', 'another', 'world', 'with', 'incredible', 'imagery', '.', 'End']\n",
            "DT NN IN DT NN TO VB DT IN DT NN .\n",
            "['his', 'script', 'is', 'also', 'excellent', ',', 'slow', 'in', 'buildup', ',', 'with', 'a', 'rapid', 'conclusion', 'and', 'a', 'fantastic', 'ending', 'that', 'has', 'to', 'be', 'seen', 'to', 'be', 'believed', '.', 'End']\n",
            "DT NN VBZ -NONE- TO MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD MD\n",
            "['it', 'is', 'difficult', 'to', 'make', 'a', 'period', 'piece', 'gripping', ':', 'scorsese', ',', 'however', ',', 'does', 'it', 'beautifully', '.', 'End']\n",
            "PRP VBZ -NONE- TO VB DT JJ NNS IN DT NN , NNP , NNP PRP VBD .\n",
            "['the', 'famous', 'cameras', 'of', 'the', 'legendary', 'director', 'are', 'also', 'everywhere', '.', 'End']\n",
            "DT NN IN IN DT NN NN , NNP NNP .\n",
            "['he', 'is', 'patient', ',', 'but', 'he', 'films', 'everything', 'and', 'anything', 'remotely', 'important', '.', 'End']\n",
            "DT VBZ -NONE- , NNP NNP NNP NNP CC NNP NNP NNP .\n",
            "['the', 'cameras', 'sweep', ',', 'pan', ',', 'track', ',', 'and', 'do', 'more', 'than', 'they', \"'\", 've', 'ever', 'done', ',', 'but', 'they', 'are', 'so', 'subtle', ',', 'one', 'doesn', \"'\", 't', 'realize', 'he', \"'\", 's', 'watching', 'all', 'the', 'scorsese', 'hallmarks', 'until', 'a', '2nd', 'viewing', '.', 'End']\n",
            "DT NN IN , NNP , NNP , CC NNP JJR IN DT NN IN DT NN , NNP NNP NNP NNP NNP , NNP NNP NNP NNP NNP NNP NNP NNP NNP NNP DT NN IN DT DT NN IN .\n",
            "['the', 'central', 'tracking', 'shot', 'is', 'probably', 'longer', 'and', 'more', 'complex', 'than', 'the', 'famous', 'goodfellas', 'shot', ',', 'but', 'the', 'viewer', 'doesn', \"'\", 't', 'notice', 'it', ',', 'because', 'we', 'want', 'to', 'see', 'more', 'of', 'this', 'gorgeous', 'world', '.', 'End']\n",
            "DT NN IN DT VBZ -NONE- TO JJ RBR IN IN DT NN IN DT , NNP DT NN IN DT NN IN PRP , NNP NNP NNP TO DT JJR IN DT NN IN .\n",
            "['there', 'are', 'a', 'few', 'deft', 'touches', 'of', 'filmmaking', 'that', 'are', 'simply', 'outstanding', ',', 'and', 'joanne', 'woodward', \"'\", 'narration', 'is', 'exquisite', '.', 'End']\n",
            "DT JJ DT NN IN DT IN DT IN DT NN IN , CC NNP NNP NNP NNP VBZ -NONE- .\n",
            "['not', 'a', 'fast', 'film', 'like', 'goodfellas', ',', 'this', 'shares', 'more', 'in', 'common', 'with', 'kundun', 'than', 'anything', 'else', '.', 'End']\n",
            "DT DT NN IN DT NN , DT NN JJR IN DT IN DT IN DT NN .\n",
            "['and', 'like', 'kundun', ',', 'this', 'is', 'a', 'slow', '-', 'starting', 'film', 'that', 'truly', 'shines', ',', 'when', 'given', 'the', 'chance', 'to', 'fully', 'breathe', 'and', 'bloom', 'in', 'the', 'end', '.', 'End']\n",
            "CC NNP NNP , DT VBZ DT NN IN DT JJ IN DT NN , NNP NNP DT JJ TO VB DT CC NNP IN DT JJ .\n",
            "['a', 'beautiful', 'film', 'by', 'a', 'director', 'continuing', 'to', 'challenge', 'himself', 'year', 'after', 'year', '.', 'End']\n",
            "DT NN IN DT DT NN IN TO DT NN NN NN NN .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS Tagging Movie Dataset using NLTK POS Tagger"
      ],
      "metadata": {
        "id": "opldzSDO2f_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_tagged_sentences = []                                                       # Generating pos tags for the entire movie dataset\n",
        "for sent in processed_text:\n",
        "  tokens = sent_tokenize(sent)\n",
        "  tokens = word_tokenize(sent)\n",
        "  pos_tagged_sentences.append(pos_tag(tokens))\n",
        "\n",
        "sent_pos_tags = [\" \".join([tag for _, tag in tags]) for tags in pos_tagged_sentences]"
      ],
      "metadata": {
        "id": "tlhY3D8-zd0u"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Strategy and Explanation for Part 3\n",
        "\n",
        "# 1.\tGenerate the POS tags for the dataset\n",
        "# 2.\tUse train-test-split to split texts as well as pos_tags into the following classes: -\n",
        "# a.\tX_train_text, X_val_text, X_test_text , y_train, y_test, y_val\n",
        "# b.\tX_train_tags, X_val_tags, X_test_tags\n",
        "# 3.\tUse 2 tf-idf vectorizers, one for embedding the texts and another for embedding the tags\n",
        "# 4.\tStack the POS tags with their corresponding sentences to create a feature vector\n",
        "# a.\tX_train_combined = [X_train_text_tfidf, X_train_tags_tfidf]\n",
        "# b.\tX_val_combined = [X_val_text_tfidf, X_ val _tags_tfidf]\n",
        "# c.\tX_test_combined = [X_ test _text_tfidf, X_ test _tags_tfidf]\n",
        "# 5.\tFinally, run the same model on these vectors.\n",
        "\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test = train_test_split(processed_text, labels, test_size=0.4, random_state=42)\n",
        "X_val_text, X_test_text, y_val, y_test = train_test_split(X_test_text, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "X_train_tags, X_test_tags = train_test_split(sent_pos_tags, test_size=0.4, random_state=42)\n",
        "X_val_tags, X_test_tags = train_test_split(X_test_tags, test_size=0.5, random_state=42)\n",
        "\n",
        "# Create TF-IDF vectorizers for text and POS tags\n",
        "text_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "pos_vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the text data\n",
        "X_train_text_tfidf = text_vectorizer.fit_transform([sent for sent in X_train_text])\n",
        "X_val_text_tfidf = text_vectorizer.transform([sent for sent in X_val_text])\n",
        "X_test_text_tfidf = text_vectorizer.transform([sent for sent in X_test_text])\n",
        "\n",
        "# Fit and transform the POS tag data\n",
        "X_train_pos_tfidf = pos_vectorizer.fit_transform([tags for tags in X_train_tags])\n",
        "X_val_pos_tfidf = pos_vectorizer.transform([tags for tags in X_val_tags])\n",
        "X_test_pos_tfidf = pos_vectorizer.transform([tags for tags in X_test_tags])\n",
        "\n",
        "# Combine the TF-IDF vectors for text and POS tags\n",
        "X_train_combined = pd.concat([pd.DataFrame(X_train_text_tfidf.toarray()), pd.DataFrame(X_train_pos_tfidf.toarray())], axis=1) # The pos tags are vertically stacked beside their corresponding sentences as features[i] = [sentence[i], pos_tag[i]]\n",
        "X_val_combined = pd.concat([pd.DataFrame(X_val_text_tfidf.toarray()), pd.DataFrame(X_val_pos_tfidf.toarray())], axis=1)\n",
        "X_test_combined = pd.concat([pd.DataFrame(X_test_text_tfidf.toarray()), pd.DataFrame(X_test_pos_tfidf.toarray())], axis=1)\n",
        "\n",
        "# Train a Support Vector Classifier (SVC)\n",
        "classifier = SVC()\n",
        "classifier.fit(X_train_combined, y_train)\n",
        "\n",
        "val_predictions = classifier.predict(X_val_combined)\n",
        "val_accuracy = accuracy_score(y_val, val_predictions)\n",
        "val_classification_report = classification_report(y_val, val_predictions)\n",
        "\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "print(\"Validation Classification Report:\\n\", val_classification_report)\n",
        "\n",
        "test_predictions = classifier.predict(X_test_combined)\n",
        "\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "test_classification_report = classification_report(y_test, test_predictions)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "print(\"Test Classification Report:\\n\", test_classification_report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AV9kKNWCEf8p",
        "outputId": "dcc3d650-f5f1-4da3-e747-fbb75834022f"
      },
      "execution_count": 197,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.82\n",
            "Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.83      0.83      0.83       211\n",
            "         pos       0.81      0.80      0.81       189\n",
            "\n",
            "    accuracy                           0.82       400\n",
            "   macro avg       0.82      0.82      0.82       400\n",
            "weighted avg       0.82      0.82      0.82       400\n",
            "\n",
            "Test Accuracy: 0.8125\n",
            "Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         neg       0.80      0.81      0.80       190\n",
            "         pos       0.83      0.81      0.82       210\n",
            "\n",
            "    accuracy                           0.81       400\n",
            "   macro avg       0.81      0.81      0.81       400\n",
            "weighted avg       0.81      0.81      0.81       400\n",
            "\n"
          ]
        }
      ]
    }
  ]
}